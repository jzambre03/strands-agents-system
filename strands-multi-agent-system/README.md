# Configuration Drift Detection System

A multi-agent AI system for detecting and analyzing configuration drift using AWS Bedrock, GitLab integration, and policy-aware analysis.

## ğŸ¯ Overview

This system validates configuration changes against a "Golden Config" (single source of truth) stored in GitLab, detects drift, enforces organizational policies, and generates detailed validation reports.

### Key Features

- **ğŸ” Precision Drift Detection**: Uses `drift.py` for line-precise analysis with structured deltas
- **ğŸ¤– Multi-Agent Architecture**: Orchestrates specialized agents for different tasks
- **ğŸ“‹ Policy-Aware**: Enforces organization-specific rules with explicit policy violations
- **ğŸ¯ Intelligent Verdicts**: 4-level decision system (PASS, WARN, REVIEW_REQUIRED, BLOCK)
- **ğŸ“Š Comprehensive Reports**: Detailed markdown reports with actionable recommendations
- **ğŸ” GitLab Integration**: Automatic validation of merge requests
- **â˜ï¸ AWS Bedrock Powered**: Uses Claude 3.5 Sonnet and Claude 3 Haiku models

## ğŸ—ï¸ Architecture

### Three-Agent System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Supervisor Agent                        â”‚
â”‚              (Claude 3.5 Sonnet)                        â”‚
â”‚         Orchestrates validation workflow                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚
        â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Config Collector â”‚  â”‚  Diff Policy       â”‚
â”‚     Agent         â”‚  â”‚  Engine Agent      â”‚
â”‚  (Claude 3 Haiku) â”‚  â”‚  (Claude 3 Haiku)  â”‚
â”‚                   â”‚  â”‚                    â”‚
â”‚  â€¢ Clones repos   â”‚  â”‚  â€¢ AI analysis     â”‚
â”‚  â€¢ Runs drift.py  â”‚  â”‚  â€¢ Policy checks   â”‚
â”‚  â€¢ Generates      â”‚  â”‚  â€¢ Risk assessment â”‚
â”‚    context_bundle â”‚  â”‚  â€¢ Generates       â”‚
â”‚                   â”‚  â”‚    enhanced_       â”‚
â”‚                   â”‚  â”‚    analysis        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
1. GitLab Repos
      â†“
2. Config Collector â†’ context_bundle.json
   (drift.py precision analysis)
      â†“
3. Diff Engine â†’ enhanced_analysis.json + llm_output.json  â­ NEW
   (Policy-aware AI analysis with LLM format)
      â†“
4. Supervisor â†’ validation_report.md
   (Final verdict + recommendations)
```

### ğŸ¯ LLM Output Format (NEW!)

The system now generates **adjudicator-friendly output** in a standardized format:

```json
{
  "meta": { "golden": "...", "candidate": "...", "generated_at": "..." },
  "overview": { "total_files": 5, "drifted_files": 2, "total_deltas": 37 },
  "high": [...],              // Critical drifts
  "medium": [...],            // Moderate drifts
  "low": [...],               // Minor drifts
  "allowed_variance": [...]   // Policy-approved
}
```

**Key Benefits**:
- âœ… **Batch Analysis**: 1 AI call per file (90% cost reduction)
- âœ… **Risk Categorization**: Automatic sorting into 4 buckets
- âœ… **Robust Parsing**: 4-tier JSON parsing with fallback
- âœ… **API Endpoint**: `GET /api/llm-output` for UI integration

ğŸ“– **Full Documentation**: [docs/LLM_OUTPUT_FORMAT.md](docs/LLM_OUTPUT_FORMAT.md)

## ğŸ“ Project Structure

```
strands-multi-agent-system/
â”œâ”€â”€ Agents/
â”‚   â”œâ”€â”€ Supervisor/
â”‚   â”‚   â”œâ”€â”€ supervisor_agent.py      # Main orchestrator
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â””â”€â”€ workers/
â”‚       â”œâ”€â”€ config_collector/
â”‚       â”‚   â”œâ”€â”€ config_collector_agent.py  # Git + drift.py
â”‚       â”‚   â””â”€â”€ README.md
â”‚       â””â”€â”€ diff_policy_engine/
â”‚           â”œâ”€â”€ diff_engine_agent.py       # AI analysis + policies
â”‚           â”œâ”€â”€ prompts/                   # â­ NEW: LLM prompt templates
â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚           â”‚   â””â”€â”€ llm_format_prompt.py
â”‚           â””â”€â”€ README.md
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ rest_server.py               # FastAPI server
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ models.py                    # Pydantic models
â”‚   â”œâ”€â”€ config.py                    # Configuration
â”‚   â”œâ”€â”€ policies.yaml                # Policy rules
â”‚   â”œâ”€â”€ drift_analyzer/              # drift.py module
â”‚   â”‚   â”œâ”€â”€ drift.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ config_data/                     # Output directory
â”‚   â”œâ”€â”€ context_bundles/             # ConfigCollector output
â”‚   â”œâ”€â”€ enhanced_analysis/           # DiffEngine output
â”‚   â”œâ”€â”€ llm_output/                  # â­ NEW: LLM format output
â”‚   â”œâ”€â”€ aggregated_results/          # Supervisor aggregated data
â”‚   â””â”€â”€ reports/                     # Final reports
â”œâ”€â”€ docs/                            # â­ NEW: Documentation
â”‚   â”œâ”€â”€ LLM_OUTPUT_FORMAT.md         # LLM format guide
â”‚   â””â”€â”€ API_REFERENCE.md             # API documentation
â”œâ”€â”€ tests/                           # â­ NEW: Test suite
â”‚   â”œâ”€â”€ test_llm_format.py           # Unit tests
â”‚   â””â”€â”€ test_integration_llm.py      # Integration tests
â”œâ”€â”€ test_pipeline.py                 # End-to-end test script
â”œâ”€â”€ validate_policies.py             # Policy validation
â”œâ”€â”€ POLICY_CUSTOMIZATION_GUIDE.md    # Policy guide
â”œâ”€â”€ .env                             # Environment config
â””â”€â”€ README.md                        # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11 or higher
- AWS account with Bedrock access
- GitLab repository access
- AWS credentials configured

### Installation

1. **Clone the repository**
   ```bash
   cd strands-multi-agent-system
   ```

2. **Install dependencies**
   ```bash
   # Install Strands SDK (from parent directory)
   pip install -e ../Strands-agent/sdk-python-main
   pip install -e ../Strands-agent/tools-main
   
   # Install other dependencies
   pip install boto3 gitpython python-dotenv colorlog pyyaml ruamel.yaml
   ```

3. **Configure environment**
   ```bash
   # Copy and edit .env file
   cp .env.example .env
   
   # Edit .env with your values:
   # - AWS_REGION
   # - BEDROCK_MODEL_ID
   # - GITLAB_TOKEN
   # - GITLAB_REPO_URL
   # - GOLDEN_BRANCH
   # - DRIFTED_BRANCH
   ```

4. **Validate setup**
   ```bash
   # Test policies
   python3 validate_policies.py
   
   # Test AWS credentials
   aws sts get-caller-identity
   ```

### Running Validation

#### Option 1: Programmatic (Python)

```python
from Agents.Supervisor.supervisor_agent import run_validation

result = run_validation(
    project_id="myorg/myrepo",
    mr_iid="123",
    repo_url="https://gitlab.verizon.com/saja9l7/golden_config.git",
    golden_branch="gold",
    drift_branch="feature-branch",
    target_folder=""  # Empty = analyze entire repo
)

print(f"Verdict: {result['data']['aggregated_results']['verdict']}")
print(f"Report: {result['data']['report_file']}")
```

#### Option 2: Test Script

```bash
# Edit test_pipeline.py with your repo details
python3 test_pipeline.py
```

#### Option 3: API Server

```bash
# Start the FastAPI server
python3 api/rest_server.py

# Trigger validation via POST request
curl -X POST http://localhost:8000/validate \
  -H "Content-Type: application/json" \
  -d '{
    "repo_url": "https://gitlab.verizon.com/saja9l7/golden_config.git",
    "golden_branch": "gold",
    "drift_branch": "feature-branch"
  }'
```

## âš™ï¸ Configuration

### Environment Variables (.env)

```bash
# AWS Configuration
AWS_REGION=us-west-2
BEDROCK_MODEL_ID=anthropic.claude-3-5-sonnet-20241022-v2:0

# GitLab Configuration
GITLAB_TOKEN=your-gitlab-token
DEFAULT_REPO_URL=https://gitlab.verizon.com/saja9l7/golden_config.git
DEFAULT_GOLDEN_BRANCH=gold
DEFAULT_DRIFT_BRANCH=drift

# Logging
LOG_LEVEL=INFO
```

### Policy Rules (shared/policies.yaml)

Customize organizational rules:

```yaml
# Environment-specific files (allowed to differ)
env_allow_keys:
  - application-dev.yml
  - values-staging.yaml

# Invariant rules (always enforced)
invariants:
  - name: require_tls_in_production
    locator_contains: ssl.enabled
    forbid_values: [false]
    severity: critical
```

See `POLICY_CUSTOMIZATION_GUIDE.md` for details.

## ğŸ“Š Understanding Results

### Verdict Levels

- **âœ… PASS**: No drift detected, safe to proceed
- **âš ï¸ WARN**: Low-risk changes, review recommended
- **ğŸ” REVIEW_REQUIRED**: Requires human approval
- **ğŸš« BLOCK**: Critical issues, deployment blocked

### Policy Tags

Changes are tagged based on policy evaluation:

- **`invariant_breach`**: Violates explicit rules â†’ **BLOCKED**
- **`allowed_variance`**: Expected environment difference â†’ **OK**
- **`suspect`**: Requires AI analysis â†’ **CONTEXT-DEPENDENT**

### Output Files

```
config_data/
â”œâ”€â”€ context_bundles/
â”‚   â””â”€â”€ context_bundle_20251005_123456.json      # Structured deltas with locators
â”œâ”€â”€ enhanced_analysis/
â”‚   â””â”€â”€ enhanced_analysis_20251005_123456.json   # AI analysis + verdicts
â”œâ”€â”€ llm_output/                                  # â­ NEW
â”‚   â””â”€â”€ llm_output_20251005_123456.json          # Adjudicator-friendly format
â”œâ”€â”€ aggregated_results/
â”‚   â””â”€â”€ aggregated_20251005_123456.json          # Supervisor aggregation
â””â”€â”€ reports/
    â””â”€â”€ validation_report_20251005_123456.md     # Final validation report
```

## ğŸŒ API Endpoints

The system provides RESTful API endpoints for integration:

### **Main Endpoints**

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | UI Dashboard |
| `/health` | GET | Service health check |
| `/api/info` | GET | API information |
| `/api/validate` | POST | Trigger validation |
| `/api/latest-results` | GET | Get latest validation results |
| `/api/llm-output` â­ | GET | Get LLM output format (NEW) |
| `/api/config` | GET | Get environment config |

### **Usage Examples**

```bash
# Start the server
python main.py

# Trigger validation
curl -X POST http://localhost:8000/api/validate \
  -H "Content-Type: application/json" \
  -d '{
    "repo_url": "https://gitlab.verizon.com/saja9l7/golden_config.git",
    "golden_branch": "gold",
    "drift_branch": "drift"
  }'

# Get LLM output (NEW)
curl http://localhost:8000/api/llm-output

# Get latest results
curl http://localhost:8000/api/latest-results
```

ğŸ“– **Full API Documentation**: [docs/API_REFERENCE.md](docs/API_REFERENCE.md)

## ğŸ”§ Customization

### Adding Policy Rules

Edit `shared/policies.yaml`:

```yaml
invariants:
  - name: your_custom_rule
    description: What this enforces
    locator_contains: config.key.path
    forbid_values: [value1, value2]
    severity: critical|high|medium|low
```

### Adjusting Agent Behavior

Each agent can be customized:

- **Supervisor**: `Agents/Supervisor/supervisor_agent.py`
- **Config Collector**: `Agents/workers/config_collector/config_collector_agent.py`
- **Diff Engine**: `Agents/workers/diff_policy_engine/diff_engine_agent.py`

### Environment-Specific Settings

Different strictness per environment:

```yaml
environment_rules:
  production:
    strictness: maximum
    require_approvals: true
  staging:
    strictness: high
    require_approvals: false
```

## ğŸ§ª Testing

### Unit Tests â­ NEW

```bash
# Run LLM format unit tests
python tests/test_llm_format.py

# Expected output:
# âœ… Passed: 5/5
# âœ… Prompt building
# âœ… Output validation
# âœ… Drift categories
# âœ… Risk levels
# âœ… Sample format
```

### Integration Tests â­ NEW

```bash
# Run integration tests
python tests/test_integration_llm.py

# Expected output:
# âœ… Passed: 4/4
# âœ… Prompt generation
# âœ… Structure validation
# âœ… Schema validation
# âœ… Field completeness
```

### Manual Testing

```bash
# Test with your GitLab repos
python3 test_pipeline.py

# Expected output:
# âœ… Pipeline executed
# âœ… Verdict generated
# âœ… Report created
```

### Policy Validation

```bash
# Validate policies.yaml syntax
python3 validate_policies.py

# Expected: 
# âœ… YAML syntax valid
# âœ… 22 invariant rules
# âœ… 28 environment allowlist entries
```

## ğŸ“‹ Troubleshooting

### Common Issues

1. **AWS Bedrock Access Error**
   ```
   Error: UnrecognizedClientException
   ```
   **Fix**: Request Bedrock model access in AWS Console

2. **Import Error (strands module)**
   ```
   ModuleNotFoundError: No module named 'strands'
   ```
   **Fix**: Install Strands SDK:
   ```bash
   pip install -e ../Strands-agent/sdk-python-main
   ```

3. **GitLab Authentication Failed**
   ```
   Error: 401 Unauthorized
   ```
   **Fix**: Check GITLAB_TOKEN in .env file

4. **Python Version Error**
   ```
   ERROR: Package 'strands-agents' requires Python: 3.10+
   ```
   **Fix**: Use Python 3.11+:
   ```bash
   python3.11 test_pipeline.py
   ```

## ğŸ“š Documentation

- **`POLICY_CUSTOMIZATION_GUIDE.md`**: Complete guide for policy rules
- **`Agents/Supervisor/README.md`**: Supervisor agent details
- **`Agents/workers/config_collector/README.md`**: Config collector details
- **`Agents/workers/diff_policy_engine/README.md`**: Diff engine details
- **`api/README.md`**: API server documentation
- **`shared/README.md`**: Shared utilities documentation

## ğŸ—ï¸ System Design

### Precision Analysis (drift.py)

The system uses `drift.py` for high-precision analysis:

- **Line-precise locators**: yamlpath, jsonpath, unidiff with exact line numbers
- **Structured deltas**: Each change is a separate, analyzable unit
- **Dependency analysis**: Parses npm, pip, Maven, Go dependencies
- **Specialized detectors**: Spring profiles, Jenkinsfiles, etc.

### Policy Enforcement

Hybrid approach: Explicit rules + AI context

1. **Explicit Rules** (`policies.yaml`): Fast, consistent, auditable
2. **AI Analysis**: Context-aware, handles edge cases
3. **Verdict Generation**: Combines both for intelligent decisions

### Multi-Agent Orchestration

- **Supervisor**: High-level orchestration with Claude 3.5 Sonnet
- **Workers**: Specialized tasks with Claude 3 Haiku (cost-effective)
- **File-Based Communication**: Scalable, provides audit trail

## ğŸ” Security Considerations

- Store `.env` file securely (never commit to Git)
- Use IAM roles with minimum required permissions
- Rotate GitLab tokens regularly
- Review policy rules for compliance requirements
- Audit `config_data/` outputs for sensitive data

## ğŸš§ Current Status

### âœ… Complete

- Multi-agent architecture (Phases 1-4)
- drift.py precision integration
- Policy-aware analysis
- Intelligent verdict logic
- File-based communication
- Comprehensive reporting
- Policy customization (Phase 6)
- Documentation & cleanup (Phase 7)

### â¸ï¸ Pending

- **End-to-end testing** (Phase 5 - requires AWS Bedrock access)

### ğŸ”œ Future Enhancements

- MR remediation (auto-fix generation)
- S3 storage for outputs
- Slack/email notifications
- Historical drift tracking
- Dashboard/UI

## ğŸ¤ Contributing

1. Read existing documentation
2. Test changes with `test_pipeline.py`
3. Validate policies with `validate_policies.py`
4. Update relevant README files
5. Follow existing code patterns

## ğŸ“ License

[Add your license information]

## ğŸ’¡ Support

For issues or questions:
1. Check troubleshooting section above
2. Review documentation in each folder
3. Validate your configuration
4. Test with `test_pipeline.py`

---

**Built with:**
- AWS Strands Multi-Agent Framework
- AWS Bedrock (Claude 3.5 Sonnet & Claude 3 Haiku)
- drift.py precision analysis
- GitLab integration
- Policy-driven validation

**Last Updated:** 2025-10-05
